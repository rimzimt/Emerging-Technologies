{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydsPE-aN1Alm"
   },
   "source": [
    "##### Copyright &copy; 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCSByGH6C7zS"
   },
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TyrY7lV0oke"
   },
   "source": [
    "# Create a TFX pipeline using templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLYriYe10okf"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This document will provide instructions to create a TensorFlow Extended (TFX) pipeline\n",
    "using *templates* which are provided with TFX Python package.\n",
    "Many of the instructions are Linux shell commands, which will run on an AI Platform Notebooks instance. Corresponding Jupyter Notebook code cells which invoke those commands using `!` are provided.\n",
    "\n",
    "You will build a pipeline using [Taxi Trips dataset](\n",
    "https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew)\n",
    "released by the City of Chicago. We strongly encourage you to try building\n",
    "your own pipeline using your dataset by utilizing this pipeline as a baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxPMeugQ0okg"
   },
   "source": [
    "## Step 1. Set up your environment.\n",
    "\n",
    "AI Platform Pipelines will prepare a development environment to build a pipeline, and a Kubeflow Pipeline cluster to run the newly built pipeline.\n",
    "\n",
    "**NOTE:** To select a particular TensorFlow version, or select a GPU instance, create a TensorFlow pre-installed instance in AI Platform Notebooks.\n",
    "\n",
    "**NOTE:** There might be some errors during package installation. For example: \n",
    "\n",
    ">\"ERROR: some-package 0.some_version.1 has requirement other-package!=2.0.,&lt;3,&gt;=1.15, but you'll have other-package 2.0.0 which is incompatible.\" Please ignore these errors at this moment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-am1yWXt0okh"
   },
   "source": [
    "Install `tfx`, `kfp`, and `skaffold`, and add installation path to the `PATH` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNiqq_kN0okj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script plasma_store is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script gen_client is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tfx is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "witwidget 1.7.0 requires oauth2client>=4.1.3, but you'll have oauth2client 3.0.0 which is incompatible.\n",
      "tensorflow 2.3.1 requires numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.4 which is incompatible.\n",
      "tensorflow-io 0.11.0 requires tensorflow==2.1.0, but you'll have tensorflow 2.3.1 which is incompatible.\n",
      "pandas-profiling 2.8.0 requires visions[type_image_path]==0.4.4, but you'll have visions 0.6.4 which is incompatible.\n",
      "explainable-ai-sdk 1.1.0 requires numpy<1.19.0, but you'll have numpy 1.19.4 which is incompatible.\n",
      "apache-beam 2.25.0 requires httplib2<0.18.0,>=0.8, but you'll have httplib2 0.18.1 which is incompatible.\u001b[0m\n",
      "\u001b[33m  WARNING: The script strip-hints is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts dsl-compile and kfp are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 47.2M  100 47.2M    0     0  90.3M      0 --:--:-- --:--:-- --:--:-- 90.3M\n"
     ]
    }
   ],
   "source": [
    "# Install tfx and kfp Python packages.\n",
    "import sys\n",
    "!{sys.executable} -m pip install --user --upgrade -q tfx==0.23.0\n",
    "!{sys.executable} -m pip install --user --upgrade -q kfp==1.0.0\n",
    "# Download skaffold and set it executable.\n",
    "!curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 && chmod +x skaffold && mv skaffold /home/jupyter/.local/bin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43ncix2Q0okm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include user python binary directory and a directory containing `skaffold`.\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hX1rqpbQ0okp"
   },
   "source": [
    "Let's check the versions of TFX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XAIoKMNG0okq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFX version: 0.23.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import tfx; print('TFX version: {}'.format(tfx.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7JLpaXT0okv"
   },
   "source": [
    "In AI Platform Pipelines, TFX is running in a hosted Kubernetes environment using [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/).\n",
    "\n",
    "Let's set some environment variables to use Kubeflow Pipelines.\n",
    "\n",
    "First, get your GCP project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hw3nsooU0okv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_CLOUD_PROJECT=wise-key-298220\n",
      "GCP project ID:wise-key-298220\n"
     ]
    }
   ],
   "source": [
    "# Read GCP project id from env.\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "GOOGLE_CLOUD_PROJECT=shell_output[0]\n",
    "%env GOOGLE_CLOUD_PROJECT={GOOGLE_CLOUD_PROJECT}\n",
    "print(\"GCP project ID:\" + GOOGLE_CLOUD_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_6r4uzE0oky"
   },
   "source": [
    "We also need to access your KFP cluster. You can access it in your Google Cloud Console under \"AI Platform > Pipeline\" menu. The \"endpoint\" of the KFP cluster can be found from the URL of the Pipelines dashboard, or you can get it from the URL of the Getting Started page where you launched this notebook. Let's create an `ENDPOINT` environment variable and set it to the KFP cluster endpoint. **ENDPOINT should contain only the hostname part of the URL.** For example, if the URL of the KFP dashboard is `https://1e9deb537390ca22-dot-asia-east1.pipelines.googleusercontent.com/#/start`, ENDPOINT value becomes `1e9deb537390ca22-dot-asia-east1.pipelines.googleusercontent.com`.\n",
    "\n",
    ">**NOTE: You MUST set your ENDPOINT value below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzqEQORV0oky"
   },
   "outputs": [],
   "source": [
    "# This refers to the KFP cluster endpoint\n",
    "ENDPOINT='https://6f1d598dc9d8bc9c-dot-us-central2.pipelines.googleusercontent.com/#/start' # Enter your ENDPOINT here.\n",
    "if not ENDPOINT:\n",
    "    from absl import logging\n",
    "    logging.error('Set your ENDPOINT in this cell.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6T-KXeA0ok3"
   },
   "source": [
    "Set the image name as `tfx-pipeline` under the current GCP project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ztxXOVD0ok4"
   },
   "outputs": [],
   "source": [
    "# Docker image name for the pipeline image.\n",
    "CUSTOM_TFX_IMAGE='gcr.io/' + GOOGLE_CLOUD_PROJECT + '/tfx-pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOsQbkky0ok7"
   },
   "source": [
    "And, it's done. We are ready to create a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cxlbi1QM0ok8"
   },
   "source": [
    "## Step 2. Copy the predefined template to your project directory.\n",
    "\n",
    "In this step, we will create a working pipeline project directory and files by copying additional files from a predefined template.\n",
    "\n",
    "You may give your pipeline a different name by changing the `PIPELINE_NAME` below. This will also become the name of the project directory where your files will be put."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIPlt-700ok-"
   },
   "outputs": [],
   "source": [
    "PIPELINE_NAME=\"my_pipeline\"\n",
    "import os\n",
    "PROJECT_DIR=os.path.join(os.path.expanduser(\"~\"),\"imported\",PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ozHIomcd0olB"
   },
   "source": [
    "TFX includes the `taxi` template with the TFX python package. If you are planning to solve a point-wise prediction problem, including classification and regresssion, this template could be used as a starting point.\n",
    "\n",
    "The `tfx template copy` CLI command copies predefined template files into your project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLXpTTjU0olD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 23:07:53.076477: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:07:53.076529: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Copying taxi pipeline template\n",
      "__init__.py -> /home/jupyter/imported/my_pipeline/__init__.py\n",
      "__init__.py -> /home/jupyter/imported/my_pipeline/models/keras/__init__.py\n",
      "model.py -> /home/jupyter/imported/my_pipeline/models/keras/model.py\n",
      "model_test.py -> /home/jupyter/imported/my_pipeline/models/keras/model_test.py\n",
      "constants.py -> /home/jupyter/imported/my_pipeline/models/keras/constants.py\n",
      "__init__.py -> /home/jupyter/imported/my_pipeline/models/__init__.py\n",
      "preprocessing.py -> /home/jupyter/imported/my_pipeline/models/preprocessing.py\n",
      "features.py -> /home/jupyter/imported/my_pipeline/models/features.py\n",
      "__init__.py -> /home/jupyter/imported/my_pipeline/models/estimator/__init__.py\n",
      "model.py -> /home/jupyter/imported/my_pipeline/models/estimator/model.py\n",
      "model_test.py -> /home/jupyter/imported/my_pipeline/models/estimator/model_test.py\n",
      "constants.py -> /home/jupyter/imported/my_pipeline/models/estimator/constants.py\n",
      "features_test.py -> /home/jupyter/imported/my_pipeline/models/features_test.py\n",
      "preprocessing_test.py -> /home/jupyter/imported/my_pipeline/models/preprocessing_test.py\n",
      "pipeline.py -> /home/jupyter/imported/my_pipeline/pipeline/pipeline.py\n",
      "configs.py -> /home/jupyter/imported/my_pipeline/pipeline/configs.py\n",
      "__init__.py -> /home/jupyter/imported/my_pipeline/pipeline/__init__.py\n",
      "kubeflow_dag_runner.py -> /home/jupyter/imported/my_pipeline/kubeflow_dag_runner.py\n",
      ".gitignore -> /home/jupyter/imported/my_pipeline/.gitignore\n",
      "model_analysis.ipynb -> /home/jupyter/imported/my_pipeline/model_analysis.ipynb\n",
      "data_validation.ipynb -> /home/jupyter/imported/my_pipeline/data_validation.ipynb\n",
      "beam_dag_runner.py -> /home/jupyter/imported/my_pipeline/beam_dag_runner.py\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx template copy \\\n",
    "  --pipeline-name={PIPELINE_NAME} \\\n",
    "  --destination-path={PROJECT_DIR} \\\n",
    "  --model=taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxOT19QS0olH"
   },
   "source": [
    "Change the working directory context in this notebook to the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6P-HljcU0olI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/imported/my_pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd {PROJECT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tEYUQxH0olO"
   },
   "source": [
    ">NOTE: Don't forget to change directory in `File Browser` on the left by clicking into the project directory once it is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IzT2PFrN0olQ"
   },
   "source": [
    "## Step 3. Browse your copied source files\n",
    "\n",
    "The TFX template provides basic scaffold files to build a pipeline, including Python source code, sample data, and Jupyter Notebooks to analyse the output of the pipeline. The `taxi` template uses the same *Chicago Taxi* dataset and ML model as the [Airflow Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/airflow_workshop).\n",
    "\n",
    "Here is brief introduction to each of the Python files.\n",
    "-   `pipeline` - This directory contains the definition of the pipeline\n",
    "    -   `configs.py` — defines common constants for pipeline runners\n",
    "    -   `pipeline.py` — defines TFX components and a pipeline\n",
    "-   `models` - This directory contains ML model definitions.\n",
    "    -   `features.py`, `features_test.py` — defines features for the model\n",
    "    -   `preprocessing.py`, `preprocessing_test.py` — defines preprocessing\n",
    "        jobs using `tf::Transform`\n",
    "    -   `estimator` - This directory contains an Estimator based model.\n",
    "        -   `constants.py` — defines constants of the model\n",
    "        -   `model.py`, `model_test.py` — defines DNN model using TF estimator\n",
    "    -   `keras` - This directory contains a Keras based model.\n",
    "        -   `constants.py` — defines constants of the model\n",
    "        -   `model.py`, `model_test.py` — defines DNN model using Keras\n",
    "-   `beam_dag_runner.py`, `kubeflow_dag_runner.py` — define runners for each orchestration engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ROwHAsDK0olT"
   },
   "source": [
    "You might notice that there are some files with `_test.py` in their name. These are unit tests of the pipeline and it is recommended to add more unit tests as you implement your own pipelines.\n",
    "You can run unit tests by supplying the module name of test files with `-m` flag. You can usually get a module name by deleting `.py` extension and replacing `/` with `.`.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0cMdE2Z0olU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 23:08:41.526428: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:08:41.526482: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Running tests under Python 3.7.8: /opt/conda/bin/python\n",
      "[ RUN      ] FeaturesTest.testNumberOfBucketFeatureBucketCount\n",
      "INFO:tensorflow:time(__main__.FeaturesTest.testNumberOfBucketFeatureBucketCount): 0.0s\n",
      "I1210 23:08:43.297006 139682405955392 test_util.py:1973] time(__main__.FeaturesTest.testNumberOfBucketFeatureBucketCount): 0.0s\n",
      "[       OK ] FeaturesTest.testNumberOfBucketFeatureBucketCount\n",
      "[ RUN      ] FeaturesTest.testTransformedNames\n",
      "INFO:tensorflow:time(__main__.FeaturesTest.testTransformedNames): 0.0s\n",
      "I1210 23:08:43.297590 139682405955392 test_util.py:1973] time(__main__.FeaturesTest.testTransformedNames): 0.0s\n",
      "[       OK ] FeaturesTest.testTransformedNames\n",
      "[ RUN      ] FeaturesTest.test_session\n",
      "[  SKIPPED ] FeaturesTest.test_session\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.001s\n",
      "\n",
      "OK (skipped=1)\n",
      "2020-12-10 23:08:44.453605: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:08:44.453656: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Running tests under Python 3.7.8: /opt/conda/bin/python\n",
      "[ RUN      ] ModelTest.testBuildKerasModel\n",
      "2020-12-10 23:08:46.275712: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:08:46.275775: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-12-10 23:08:46.275826: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-2-1-20201210-145329): /proc/driver/nvidia/version does not exist\n",
      "2020-12-10 23:08:46.276103: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-10 23:08:46.288076: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200140000 Hz\n",
      "2020-12-10 23:08:46.288847: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561a9e0792f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-10 23:08:46.288911: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "I1210 23:08:46.379731 140427316004672 layer_utils.py:192] Model: \"functional_1\"\n",
      "I1210 23:08:46.379942 140427316004672 layer_utils.py:193] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.380052 140427316004672 layer_utils.py:190] Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "I1210 23:08:46.380123 140427316004672 layer_utils.py:195] ==================================================================================================\n",
      "I1210 23:08:46.380378 140427316004672 layer_utils.py:190] pickup_latitude_xf (InputLayer) [(None,)]            0                                            \n",
      "I1210 23:08:46.380501 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.380694 140427316004672 layer_utils.py:190] trip_miles_xf (InputLayer)      [(None,)]            0                                            \n",
      "I1210 23:08:46.380792 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.380983 140427316004672 layer_utils.py:190] trip_start_hour_xf (InputLayer) [(None,)]            0                                            \n",
      "I1210 23:08:46.381068 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.381340 140427316004672 layer_utils.py:190] dense_features (DenseFeatures)  (None, 1)            0           pickup_latitude_xf[0][0]         \n",
      "I1210 23:08:46.381458 140427316004672 layer_utils.py:190]                                                                  trip_miles_xf[0][0]              \n",
      "I1210 23:08:46.381546 140427316004672 layer_utils.py:190]                                                                  trip_start_hour_xf[0][0]         \n",
      "I1210 23:08:46.381616 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.381925 140427316004672 layer_utils.py:190] dense (Dense)                   (None, 1)            2           dense_features[0][0]             \n",
      "I1210 23:08:46.382030 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.382288 140427316004672 layer_utils.py:190] dense_1 (Dense)                 (None, 1)            2           dense[0][0]                      \n",
      "I1210 23:08:46.382398 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.382681 140427316004672 layer_utils.py:190] dense_features_1 (DenseFeatures (None, 34)           0           pickup_latitude_xf[0][0]         \n",
      "I1210 23:08:46.382776 140427316004672 layer_utils.py:190]                                                                  trip_miles_xf[0][0]              \n",
      "I1210 23:08:46.382869 140427316004672 layer_utils.py:190]                                                                  trip_start_hour_xf[0][0]         \n",
      "I1210 23:08:46.382942 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.383114 140427316004672 layer_utils.py:190] concatenate (Concatenate)       (None, 35)           0           dense_1[0][0]                    \n",
      "I1210 23:08:46.383200 140427316004672 layer_utils.py:190]                                                                  dense_features_1[0][0]           \n",
      "I1210 23:08:46.383289 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.383532 140427316004672 layer_utils.py:190] dense_2 (Dense)                 (None, 1)            36          concatenate[0][0]                \n",
      "I1210 23:08:46.383636 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.383872 140427316004672 layer_utils.py:190] tf_op_layer_Squeeze (TensorFlow [(None,)]            0           dense_2[0][0]                    \n",
      "I1210 23:08:46.383974 140427316004672 layer_utils.py:257] ==================================================================================================\n",
      "I1210 23:08:46.384629 140427316004672 layer_utils.py:268] Total params: 40\n",
      "I1210 23:08:46.385219 140427316004672 layer_utils.py:269] Trainable params: 40\n",
      "I1210 23:08:46.385334 140427316004672 layer_utils.py:270] Non-trainable params: 0\n",
      "I1210 23:08:46.385416 140427316004672 layer_utils.py:271] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.480384 140427316004672 layer_utils.py:192] Model: \"functional_3\"\n",
      "I1210 23:08:46.480569 140427316004672 layer_utils.py:193] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.480668 140427316004672 layer_utils.py:190] Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "I1210 23:08:46.480735 140427316004672 layer_utils.py:195] ==================================================================================================\n",
      "I1210 23:08:46.480995 140427316004672 layer_utils.py:190] pickup_latitude_xf (InputLayer) [(None,)]            0                                            \n",
      "I1210 23:08:46.481128 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.481365 140427316004672 layer_utils.py:190] trip_miles_xf (InputLayer)      [(None,)]            0                                            \n",
      "I1210 23:08:46.481464 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.481652 140427316004672 layer_utils.py:190] trip_start_hour_xf (InputLayer) [(None,)]            0                                            \n",
      "I1210 23:08:46.481732 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.481981 140427316004672 layer_utils.py:190] dense_features_2 (DenseFeatures (None, 1)            0           pickup_latitude_xf[0][0]         \n",
      "I1210 23:08:46.482090 140427316004672 layer_utils.py:190]                                                                  trip_miles_xf[0][0]              \n",
      "I1210 23:08:46.482178 140427316004672 layer_utils.py:190]                                                                  trip_start_hour_xf[0][0]         \n",
      "I1210 23:08:46.482256 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.482538 140427316004672 layer_utils.py:190] dense_3 (Dense)                 (None, 1)            2           dense_features_2[0][0]           \n",
      "I1210 23:08:46.482639 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.482882 140427316004672 layer_utils.py:190] dense_features_3 (DenseFeatures (None, 34)           0           pickup_latitude_xf[0][0]         \n",
      "I1210 23:08:46.482992 140427316004672 layer_utils.py:190]                                                                  trip_miles_xf[0][0]              \n",
      "I1210 23:08:46.483078 140427316004672 layer_utils.py:190]                                                                  trip_start_hour_xf[0][0]         \n",
      "I1210 23:08:46.483391 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.483727 140427316004672 layer_utils.py:190] concatenate_1 (Concatenate)     (None, 35)           0           dense_3[0][0]                    \n",
      "I1210 23:08:46.483880 140427316004672 layer_utils.py:190]                                                                  dense_features_3[0][0]           \n",
      "I1210 23:08:46.483971 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.484247 140427316004672 layer_utils.py:190] dense_4 (Dense)                 (None, 1)            36          concatenate_1[0][0]              \n",
      "I1210 23:08:46.484368 140427316004672 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I1210 23:08:46.484646 140427316004672 layer_utils.py:190] tf_op_layer_Squeeze_1 (TensorFl [(None,)]            0           dense_4[0][0]                    \n",
      "I1210 23:08:46.484736 140427316004672 layer_utils.py:257] ==================================================================================================\n",
      "I1210 23:08:46.485294 140427316004672 layer_utils.py:268] Total params: 38\n",
      "I1210 23:08:46.485396 140427316004672 layer_utils.py:269] Trainable params: 38\n",
      "I1210 23:08:46.485481 140427316004672 layer_utils.py:270] Non-trainable params: 0\n",
      "I1210 23:08:46.485553 140427316004672 layer_utils.py:271] __________________________________________________________________________________________________\n",
      "INFO:tensorflow:time(__main__.ModelTest.testBuildKerasModel): 0.24s\n",
      "I1210 23:08:46.486206 140427316004672 test_util.py:1973] time(__main__.ModelTest.testBuildKerasModel): 0.24s\n",
      "[       OK ] ModelTest.testBuildKerasModel\n",
      "[ RUN      ] ModelTest.test_session\n",
      "[  SKIPPED ] ModelTest.test_session\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.236s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m models.features_test\n",
    "!{sys.executable} -m models.keras.model_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tO9Jhplo0olX"
   },
   "source": [
    "## Step 4. Run your first TFX pipeline\n",
    "\n",
    "Components in the TFX pipeline will generate outputs for each run as [ML Metadata Artifacts](https://www.tensorflow.org/tfx/guide/mlmd), and they need to be stored somewhere. You can use any storage which the KFP cluster can access, and for this example we will use Google Cloud Storage (GCS). A default GCS bucket should have been created automatically. Its name will be `<your-project-id>-kubeflowpipelines-default`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zr-RjyPWTHdH"
   },
   "source": [
    "Let's upload our sample data to GCS bucket so that we can use it in our pipeline later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gW-dSHW-TSdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/data.csv [Content-Type=text/csv]...\n",
      "/ [1 files][  1.9 MiB/  1.9 MiB]                                                \n",
      "Operation completed over 1 objects/1.9 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp data/data.csv gs://{GOOGLE_CLOUD_PROJECT}-kubeflowpipelines-default/tfx-template/data/data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wc54hDZu0ole"
   },
   "source": [
    "Let's create a TFX pipeline using the `tfx pipeline create` command.\n",
    "\n",
    ">Note: When creating a pipeline for KFP, we need a container image which will be used to run our pipeline. And `skaffold` will build the image for us. Because skaffold pulls base images from the docker hub, it will take 5~10 minutes when we build the image for the first time, but it will take much less time from the second build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOU7zQof0olf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 23:15:20.870644: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:15:20.870697: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "Target image gcr.io/wise-key-298220/tfx-pipeline is not used. If the build spec is provided, update the target image in the build spec file build.yaml.\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/wise-key-298220/tfx-pipeline -> gcr.io/wise-key-298220/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/wise-key-298220/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Building [gcr.io/wise-key-298220/tfx-pipeline]...\n",
      "Sending build context to Docker daemon  2.066MBon    534kB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.23.0\n",
      "[Skaffold]  ---> f8355268ae44\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> 442cedb2b308\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> e01ea2285365\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in 363ab2c00c2b\n",
      "[Skaffold]  ---> ba5c53c48b02\n",
      "[Skaffold] Successfully built ba5c53c48b02\n",
      "[Skaffold] Successfully tagged gcr.io/wise-key-298220/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/wise-key-298220/tfx-pipeline]\n",
      "[Skaffold] d897a7e28378: Preparing\n",
      "[Skaffold] 291bb236dc3f: Preparing\n",
      "[Skaffold] 9042e84a180f: Preparing\n",
      "[Skaffold] 4f874315440c: Preparing\n",
      "[Skaffold] f13a156e38fd: Preparing\n",
      "[Skaffold] 572b4b19939e: Preparing\n",
      "[Skaffold] d8bc156b2e76: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 572b4b19939e: Waiting\n",
      "[Skaffold] d8bc156b2e76: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] 4f874315440c: Layer already exists\n",
      "[Skaffold] f13a156e38fd: Layer already exists\n",
      "[Skaffold] 291bb236dc3f: Layer already exists\n",
      "[Skaffold] 9042e84a180f: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] d8bc156b2e76: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 572b4b19939e: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] d897a7e28378: Pushed\n",
      "[Skaffold] latest: digest: sha256:c34b0dd39eb2f22171e9c234bfb3046948af30bd799365e31aa87cd4df3ba271 size: 3479\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2020-12-10 23:15:39.838917: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:15:39.838972: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "WARNING:tensorflow:From /home/jupyter/imported/my_pipeline/pipeline/pipeline.py:75: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:absl:The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:Only one of `artifacts` and `matching_channel_name` should be set.\n",
      "INFO:absl:Adding upstream dependencies for component CsvExampleGen\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/imported/my_pipeline/my_pipeline.tar.gz\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/tfx\", line 10, in <module>\n",
      "    sys.exit(cli_group())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 829, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 782, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 1259, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 1259, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 1066, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 610, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/click/decorators.py\", line 73, in new_func\n",
      "    return ctx.invoke(f, obj, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 610, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/tools/cli/commands/pipeline.py\", line 117, in create_pipeline\n",
      "    handler_factory.create_handler(ctx.flags_dict).create_pipeline()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/tools/cli/handler/kubeflow_handler.py\", line 91, in create_pipeline\n",
      "    self._save_pipeline(pipeline_args, update=update)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/tools/cli/handler/kubeflow_handler.py\", line 240, in _save_pipeline\n",
      "    pipeline_name=pipeline_name)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kfp/_client.py\", line 720, in upload_pipeline\n",
      "    response = self._upload_api.upload_pipeline(pipeline_package_path, name=pipeline_name, description=description)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kfp_server_api/api/pipeline_upload_service_api.py\", line 83, in upload_pipeline\n",
      "    return self.upload_pipeline_with_http_info(uploadfile, **kwargs)  # noqa: E501\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kfp_server_api/api/pipeline_upload_service_api.py\", line 191, in upload_pipeline_with_http_info\n",
      "    collection_formats=collection_formats)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kfp_server_api/api_client.py\", line 383, in call_api\n",
      "    _preload_content, _request_timeout, _host)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kfp_server_api/api_client.py\", line 202, in __call_api\n",
      "    raise e\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kfp_server_api/api_client.py\", line 199, in __call_api\n",
      "    _request_timeout=_request_timeout)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kfp_server_api/api_client.py\", line 427, in request\n",
      "    body=body)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kfp_server_api/rest.py\", line 285, in POST\n",
      "    body=body)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kfp_server_api/rest.py\", line 238, in request\n",
      "    raise ApiException(http_resp=r)\n",
      "kfp_server_api.exceptions.ApiException: (403)\n",
      "Reason: Forbidden\n",
      "HTTP response headers: HTTPHeaderDict({'Content-Length': '1449', 'Content-Type': 'text/html; charset=utf-8', 'Date': 'Thu, 10 Dec 2020 23:15:43 GMT', 'Vary': 'Origin', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'SAMEORIGIN', 'X-Xss-Protection': '0', 'Set-Cookie': 'S=cloud_datalab_tunnel=AZ_1xleR-054KnLmqzBjrRvBhC4MV_IhvZnR8grzGYE; Path=/; Max-Age=3600'})\n",
      "HTTP response body: \n",
      "<!DOCTYPE html>\n",
      "<html lang=en>\n",
      "  <meta charset=utf-8>\n",
      "  <meta name=viewport content=\"initial-scale=1, minimum-scale=1, width=device-width\">\n",
      "  <title>Error 403 (Forbidden)!!1</title>\n",
      "  <style>\n",
      "    *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/logos/errorpage/error_logo-150x54.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/logos/errorpage/error_logo-150x54-2x.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/logos/errorpage/error_logo-150x54-2x.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/logos/errorpage/error_logo-150x54-2x.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}\n",
      "  </style>\n",
      "  <a href=//www.google.com/><span id=logo aria-label=Google></span></a>\n",
      "  <p><b>403.</b> <ins>That’s an error.</ins>\n",
      "  <p>  <ins>That’s all we know.</ins>\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx pipeline create  \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT} \\\n",
    "--build-target-image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmA6___Y0olh"
   },
   "source": [
    "While creating a pipeline, `Dockerfile` and `build.yaml` will be generated to build a Docker image. Don't forget to add these files to the source control system (for example, git) along with other source files.\n",
    "\n",
    "A pipeline definition file for [argo](https://argoproj.github.io/argo/) will be generated, too. The name of this file is `${PIPELINE_NAME}.tar.gz`. For example, it will be `my_pipeline.tar.gz` if the name of your pipeline is `my_pipeline`. It is recommended NOT to include this pipeline definition file into source control, because it will be generated from other Python files and will be updated whenever you update the pipeline. For your convenience, this file is already listed in `.gitignore` which is generated automatically.\n",
    "\n",
    "NOTE: `kubeflow` will be automatically selected as an orchestration engine if `airflow` is not installed and `--engine` is not specified.\n",
    "\n",
    "Now start an execution run with the newly created pipeline using the `tfx run create` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKSjVVsa0oli"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 23:17:24.502952: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:17:24.503002: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating a run for pipeline: my_pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Pipeline \"my_pipeline\" does not exist.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx run create --pipeline-name={PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pg0VxvUC0olk"
   },
   "source": [
    "Or, you can also run the pipeline in the KFP Dashboard.  The new execution run will be listed under Experiments in the KFP Dashboard.  Clicking into the experiment will allow you to monitor progress and visualize the artifacts created during the execution run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nLN4ges90oll"
   },
   "source": [
    "However, we recommend visiting the KFP Dashboard. You can access the KFP Dashboard from the Cloud AI Platform Pipelines menu in Google Cloud Console. Once you visit the dashboard, you will be able to find the pipeline, and access a wealth of information about the pipeline.\n",
    "For example, you can find your runs under the *Experiments* menu, and when you open your execution run under Experiments you can find all your artifacts from the pipeline under *Artifacts* menu.\n",
    "\n",
    ">Note: If your pipeline run fails, you can see detailed logs for each TFX component in the Experiments tab in the KFP Dashboard.\n",
    "    \n",
    "One of the major sources of failure is permission related problems. Please make sure your KFP cluster has permissions to access Google Cloud APIs. This can be configured [when you create a KFP cluster in GCP](https://cloud.google.com/ai-platform/pipelines/docs/setting-up), or see [Troubleshooting document in GCP](https://cloud.google.com/ai-platform/pipelines/docs/troubleshooting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYH8Y2KB0olm"
   },
   "source": [
    "## Step 5. Add components for data validation.\n",
    "\n",
    "In this step, you will add components for data validation including `StatisticsGen`, `SchemaGen`, and `ExampleValidator`. If you are interested in data validation, please see [Get started with Tensorflow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started).\n",
    "\n",
    ">**Double-click to change directory to `pipeline` and double-click again to open `pipeline.py`**. Find and uncomment the 3 lines which add `StatisticsGen`, `SchemaGen`, and `ExampleValidator` to the pipeline. (Tip: search for comments containing `TODO(step 5):`).  Make sure to save `pipeline.py` after you edit it.\n",
    "\n",
    "You now need to update the existing pipeline with modified pipeline definition. Use the `tfx pipeline update` command to update your pipeline, followed by the `tfx run create` command to create a new execution run of your updated pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VE-Pqvto0olm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 23:17:30.600300: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:17:30.600349: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/wise-key-298220/tfx-pipeline -> gcr.io/wise-key-298220/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/wise-key-298220/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Building [gcr.io/wise-key-298220/tfx-pipeline]...\n",
      "Sending build context to Docker daemon  2.066MBon    534kB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.23.0\n",
      "[Skaffold]  ---> f8355268ae44\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> 442cedb2b308\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> 7bf35f1cbac0\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in 06bbbe67b9a7\n",
      "[Skaffold]  ---> b63e330342d4\n",
      "[Skaffold] Successfully built b63e330342d4\n",
      "[Skaffold] Successfully tagged gcr.io/wise-key-298220/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/wise-key-298220/tfx-pipeline]\n",
      "[Skaffold] 2841d040e45c: Preparing\n",
      "[Skaffold] 291bb236dc3f: Preparing\n",
      "[Skaffold] 9042e84a180f: Preparing\n",
      "[Skaffold] 4f874315440c: Preparing\n",
      "[Skaffold] f13a156e38fd: Preparing\n",
      "[Skaffold] 572b4b19939e: Preparing\n",
      "[Skaffold] d8bc156b2e76: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] 572b4b19939e: Waiting\n",
      "[Skaffold] d8bc156b2e76: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] f13a156e38fd: Layer already exists\n",
      "[Skaffold] 4f874315440c: Layer already exists\n",
      "[Skaffold] 291bb236dc3f: Layer already exists\n",
      "[Skaffold] 9042e84a180f: Layer already exists\n",
      "[Skaffold] d8bc156b2e76: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] 572b4b19939e: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] 2841d040e45c: Pushed\n",
      "[Skaffold] latest: digest: sha256:0c4cc2f46ef1b8191468fe6e9e934e883f4e4c85ebadcb3de9d287ed84d54bc9 size: 3479\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2020-12-10 23:17:49.581611: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:17:49.581663: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "WARNING:tensorflow:From /home/jupyter/imported/my_pipeline/pipeline/pipeline.py:75: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:absl:The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:Only one of `artifacts` and `matching_channel_name` should be set.\n",
      "INFO:absl:Adding upstream dependencies for component CsvExampleGen\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/imported/my_pipeline/my_pipeline.tar.gz\n",
      "Pipeline \"my_pipeline\" does not exist.\n",
      "\u001b[0m2020-12-10 23:17:54.571994: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:17:54.572048: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating a run for pipeline: my_pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Pipeline \"my_pipeline\" does not exist.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Update the pipeline\n",
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "# You can run the pipeline the same way.\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8q1ZYEHX0olo"
   },
   "source": [
    "### Check pipeline outputs\n",
    "\n",
    "Visit the KFP dashboard to find pipeline outputs in the page for your pipeline run. Click the *Experiments* tab on the left, and *All runs* in the Experiments page. You should be able to find the latest run under the name of your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWMBXU510olp"
   },
   "source": [
    "## Step 6. Add components for training.\n",
    "\n",
    "In this step, you will add components for training and model validation including `Transform`, `Trainer`, `ResolverNode`, `Evaluator`, and `Pusher`.\n",
    "\n",
    ">**Double-click to open `pipeline.py`**. Find and uncomment the 5 lines which add `Transform`, `Trainer`, `ResolverNode`, `Evaluator` and `Pusher` to the pipeline. (Tip: search for `TODO(step 6):`)\n",
    "\n",
    "As you did before, you now need to update the existing pipeline with the modified pipeline definition. The instructions are the same as Step 5. Update the pipeline using `tfx pipeline update`, and create an execution run using `tfx run create`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQDNitkH0olq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 23:20:20.737497: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:20:20.737551: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/wise-key-298220/tfx-pipeline -> gcr.io/wise-key-298220/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/wise-key-298220/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Building [gcr.io/wise-key-298220/tfx-pipeline]...\n",
      "Sending build context to Docker daemon  2.066MBon    534kB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.23.0\n",
      "[Skaffold]  ---> f8355268ae44\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> 442cedb2b308\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> 4bebd6af4aa0\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in ddc0aa23fa1f\n",
      "[Skaffold]  ---> 11309ed54508\n",
      "[Skaffold] Successfully built 11309ed54508\n",
      "[Skaffold] Successfully tagged gcr.io/wise-key-298220/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/wise-key-298220/tfx-pipeline]\n",
      "[Skaffold] f41b100cbc38: Preparing\n",
      "[Skaffold] 291bb236dc3f: Preparing\n",
      "[Skaffold] 9042e84a180f: Preparing\n",
      "[Skaffold] 4f874315440c: Preparing\n",
      "[Skaffold] f13a156e38fd: Preparing\n",
      "[Skaffold] 572b4b19939e: Preparing\n",
      "[Skaffold] d8bc156b2e76: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 572b4b19939e: Waiting\n",
      "[Skaffold] d8bc156b2e76: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] 4f874315440c: Layer already exists\n",
      "[Skaffold] 291bb236dc3f: Layer already exists\n",
      "[Skaffold] 9042e84a180f: Layer already exists\n",
      "[Skaffold] f13a156e38fd: Layer already exists\n",
      "[Skaffold] d8bc156b2e76: Layer already exists\n",
      "[Skaffold] 572b4b19939e: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] f41b100cbc38: Pushed\n",
      "[Skaffold] latest: digest: sha256:fd8630e03fc2d8583b778b0bc68020cfae90728364d875717996c942a715566f size: 3479\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2020-12-10 23:20:39.566292: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:20:39.566350: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "WARNING:tensorflow:From /home/jupyter/imported/my_pipeline/pipeline/pipeline.py:75: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:absl:The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:Only one of `artifacts` and `matching_channel_name` should be set.\n",
      "INFO:absl:Adding upstream dependencies for component CsvExampleGen\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/imported/my_pipeline/my_pipeline.tar.gz\n",
      "Pipeline \"my_pipeline\" does not exist.\n",
      "\u001b[0m2020-12-10 23:20:44.707717: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:20:44.707772: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating a run for pipeline: my_pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Pipeline \"my_pipeline\" does not exist.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksWfVQUnMYCX"
   },
   "source": [
    "When this execution run finishes successfully, you have now created and run your first TFX pipeline in AI Platform Pipelines!\n",
    "\n",
    "**NOTE:** You might have noticed that every time we create a pipeline run, every component runs again and again even though the input and the parameters were not changed.\n",
    "It is waste of time and resources, and you can skip those executions with pipeline caching. You can enable caching by specifying `enable_cache=True` for the `Pipeline` object in `pipeline.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkF7klWi0ols"
   },
   "source": [
    "## Step 7. (*Optional*) Try BigQueryExampleGen\n",
    "\n",
    "[BigQuery](https://cloud.google.com/bigquery) is a serverless, highly scalable, and cost-effective cloud data warehouse. BigQuery can be used as a source for training examples in TFX. In this step, we will add `BigQueryExampleGen` to the pipeline.\n",
    "\n",
    ">**Double-click to open `pipeline.py`**. Comment out `CsvExampleGen` and uncomment the line which creates an instance of `BigQueryExampleGen`. You also need to uncomment the `query` argument of the `create_pipeline` function.\n",
    "\n",
    "We need to specify which GCP project to use for BigQuery, and this is done by setting `--project` in `beam_pipeline_args` when creating a pipeline.\n",
    "\n",
    ">**Double-click to open `configs.py`**. Uncomment the definition of `GOOGLE_CLOUD_REGION`, `BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS` and `BIG_QUERY_QUERY`. You should replace the region value in this file with the correct values for your GCP project.\n",
    "\n",
    ">**Note: You MUST set your GCP region in the `configs.py` file before proceeding.**\n",
    "\n",
    ">**Change directory one level up.** Click the name of the directory above the file list. The name of the directory is the name of the pipeline which is `my_pipeline` if you didn't change.\n",
    "\n",
    ">**Double-click to open `kubeflow_dag_runner.py`**. Uncomment two arguments, `query` and `beam_pipeline_args`, for the `create_pipeline` function.\n",
    "\n",
    "Now the pipeline is ready to use BigQuery as an example source. Update the pipeline as before and create a new execution run as we did in step 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1sD3NxB60olt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 23:20:57.541288: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:20:57.541346: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/wise-key-298220/tfx-pipeline -> gcr.io/wise-key-298220/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/wise-key-298220/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Building [gcr.io/wise-key-298220/tfx-pipeline]...\n",
      "Sending build context to Docker daemon  2.066MBon    534kB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.23.0\n",
      "[Skaffold]  ---> f8355268ae44\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> 442cedb2b308\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> 3e65ecd8ec05\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in 9ceb7c5081c8\n",
      "[Skaffold]  ---> 3c750be1db89\n",
      "[Skaffold] Successfully built 3c750be1db89\n",
      "[Skaffold] Successfully tagged gcr.io/wise-key-298220/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/wise-key-298220/tfx-pipeline]\n",
      "[Skaffold] 7619791dc3eb: Preparing\n",
      "[Skaffold] 291bb236dc3f: Preparing\n",
      "[Skaffold] 9042e84a180f: Preparing\n",
      "[Skaffold] 4f874315440c: Preparing\n",
      "[Skaffold] f13a156e38fd: Preparing\n",
      "[Skaffold] 572b4b19939e: Preparing\n",
      "[Skaffold] d8bc156b2e76: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 572b4b19939e: Waiting\n",
      "[Skaffold] d8bc156b2e76: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] 291bb236dc3f: Layer already exists\n",
      "[Skaffold] f13a156e38fd: Layer already exists\n",
      "[Skaffold] 9042e84a180f: Layer already exists\n",
      "[Skaffold] 4f874315440c: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] 572b4b19939e: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] d8bc156b2e76: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] 7619791dc3eb: Pushed\n",
      "[Skaffold] latest: digest: sha256:89f0a335ed5c0ea6fceca870ca4b748254d209e8ba2ad401adc18e9f2a972cdd size: 3479\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2020-12-10 23:21:17.395222: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:21:17.395277: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "WARNING:tensorflow:From /home/jupyter/imported/my_pipeline/pipeline/pipeline.py:75: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:absl:The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:Only one of `artifacts` and `matching_channel_name` should be set.\n",
      "INFO:absl:Adding upstream dependencies for component CsvExampleGen\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/imported/my_pipeline/my_pipeline.tar.gz\n",
      "Pipeline \"my_pipeline\" does not exist.\n",
      "\u001b[0m2020-12-10 23:21:22.469979: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:21:22.470033: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating a run for pipeline: my_pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Pipeline \"my_pipeline\" does not exist.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LpA2R6Lu0olv"
   },
   "source": [
    "## Step 8. (*Optional*) Try Dataflow with KFP\n",
    "\n",
    "Several [TFX Components uses Apache Beam](https://www.tensorflow.org/tfx/guide/beam) to implement data-parallel pipelines, and it means that you can distribute data processing workloads using [Google Cloud Dataflow](https://cloud.google.com/dataflow/). In this step, we will set the Kubeflow orchestrator to use dataflow as the data processing back-end for Apache Beam.\n",
    "\n",
    ">**Double-click `pipeline` to change directory, and double-click to open `configs.py`**. Uncomment the definition of `GOOGLE_CLOUD_REGION`, and `DATAFLOW_BEAM_PIPELINE_ARGS`.\n",
    "\n",
    ">**Change directory one level up.** Click the name of the directory above the file list. The name of the directory is the name of the pipeline which is `my_pipeline` if you didn't change.\n",
    "\n",
    ">**Double-click to open `kubeflow_dag_runner.py`**. Uncomment `beam_pipeline_args`. (Also make sure to comment out current `beam_pipeline_args` that you added in Step 7.)\n",
    "\n",
    "Now the pipeline is ready to use Dataflow. Update the pipeline and create an execution run as we did in step 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3HVPcKi0olw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 23:23:20.023905: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:23:20.023955: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/wise-key-298220/tfx-pipeline -> gcr.io/wise-key-298220/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/wise-key-298220/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Building [gcr.io/wise-key-298220/tfx-pipeline]...\n",
      "Sending build context to Docker daemon  2.066MBon    534kB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.23.0\n",
      "[Skaffold]  ---> f8355268ae44\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> 442cedb2b308\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> 952a11032bb5\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in b505e6959038\n",
      "[Skaffold]  ---> 8184ce2466f9\n",
      "[Skaffold] Successfully built 8184ce2466f9\n",
      "[Skaffold] Successfully tagged gcr.io/wise-key-298220/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/wise-key-298220/tfx-pipeline]\n",
      "[Skaffold] b3bc0123d6d6: Preparing\n",
      "[Skaffold] 291bb236dc3f: Preparing\n",
      "[Skaffold] 9042e84a180f: Preparing\n",
      "[Skaffold] 4f874315440c: Preparing\n",
      "[Skaffold] f13a156e38fd: Preparing\n",
      "[Skaffold] 572b4b19939e: Preparing\n",
      "[Skaffold] d8bc156b2e76: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 572b4b19939e: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] d8bc156b2e76: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] f13a156e38fd: Layer already exists\n",
      "[Skaffold] 4f874315440c: Layer already exists\n",
      "[Skaffold] 9042e84a180f: Layer already exists\n",
      "[Skaffold] 291bb236dc3f: Layer already exists\n",
      "[Skaffold] 572b4b19939e: Layer already exists\n",
      "[Skaffold] d8bc156b2e76: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] b3bc0123d6d6: Pushed\n",
      "[Skaffold] latest: digest: sha256:8f1cf1c98b173231d5a8906109c754a477016184a69740e61723bfbd918a4782 size: 3479\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2020-12-10 23:23:38.998285: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:23:38.998339: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "WARNING:tensorflow:From /home/jupyter/imported/my_pipeline/pipeline/pipeline.py:75: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:absl:The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:Only one of `artifacts` and `matching_channel_name` should be set.\n",
      "INFO:absl:Adding upstream dependencies for component CsvExampleGen\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/imported/my_pipeline/my_pipeline.tar.gz\n",
      "Pipeline \"my_pipeline\" does not exist.\n",
      "\u001b[0m2020-12-10 23:23:43.954226: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:23:43.954278: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating a run for pipeline: my_pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Pipeline \"my_pipeline\" does not exist.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uxDY13N0oly"
   },
   "source": [
    "You can find your Dataflow jobs in [Dataflow in Cloud Console](http://console.cloud.google.com/dataflow).\n",
    "\n",
    ">**Double-click to open `pipeline.py`**. Reset the value of `enable_cache` to `True`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJOmh1RY0olz"
   },
   "source": [
    "## Step 9. (*Optional*) Try Cloud AI Platform Training and Prediction with KFP\n",
    "\n",
    "TFX interoperates with several managed GCP services, such as [Cloud AI Platform for Training and Prediction](https://cloud.google.com/ai-platform/). You can set your `Trainer` component to use Cloud AI Platform Training, a managed service for training ML models. Moreover, when your model is built and ready to be served, you can *push* your model to Cloud AI Platform Prediction for serving. In this step, we will set our `Trainer` and `Pusher` component to use Cloud AI Platform services.\n",
    "\n",
    ">Before editing files, you might first have to enable *AI Platform Training & Prediction API*.\n",
    "\n",
    ">**Double-click `pipeline` to change directory, and double-click to open `configs.py`**. Uncomment the definition of `GOOGLE_CLOUD_REGION`, `GCP_AI_PLATFORM_TRAINING_ARGS` and `GCP_AI_PLATFORM_SERVING_ARGS`. We will use our custom built container image to train a model in Cloud AI Platform Training, so we should set `masterConfig.imageUri` in `GCP_AI_PLATFORM_TRAINING_ARGS` to the same value as `CUSTOM_TFX_IMAGE` above.\n",
    "\n",
    ">**Change directory one level up, and double-click to open `kubeflow_dag_runner.py`**. Uncomment `ai_platform_training_args` and `ai_platform_serving_args`.\n",
    "\n",
    "Update the pipeline and create an execution run as we did in step 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxOjhBmG0ol0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 23:26:52.292433: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:26:52.292500: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/wise-key-298220/tfx-pipeline -> gcr.io/wise-key-298220/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/wise-key-298220/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Building [gcr.io/wise-key-298220/tfx-pipeline]...\n",
      "Sending build context to Docker daemon  2.066MBon    534kB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.23.0\n",
      "[Skaffold]  ---> f8355268ae44\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> 442cedb2b308\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> ca0722952e7a\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in 3d9bdcaa2fa2\n",
      "[Skaffold]  ---> 5d1224278b54\n",
      "[Skaffold] Successfully built 5d1224278b54\n",
      "[Skaffold] Successfully tagged gcr.io/wise-key-298220/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/wise-key-298220/tfx-pipeline]\n",
      "[Skaffold] 4dfd0db93b67: Preparing\n",
      "[Skaffold] 291bb236dc3f: Preparing\n",
      "[Skaffold] 9042e84a180f: Preparing\n",
      "[Skaffold] 4f874315440c: Preparing\n",
      "[Skaffold] f13a156e38fd: Preparing\n",
      "[Skaffold] 572b4b19939e: Preparing\n",
      "[Skaffold] d8bc156b2e76: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 572b4b19939e: Waiting\n",
      "[Skaffold] d8bc156b2e76: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] 291bb236dc3f: Layer already exists\n",
      "[Skaffold] 9042e84a180f: Layer already exists\n",
      "[Skaffold] 4f874315440c: Layer already exists\n",
      "[Skaffold] f13a156e38fd: Layer already exists\n",
      "[Skaffold] d8bc156b2e76: Layer already exists\n",
      "[Skaffold] 572b4b19939e: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 4dfd0db93b67: Pushed\n",
      "[Skaffold] latest: digest: sha256:99b340c282ce6d51bb17626bce4c0b4afeef34ca9951ff2c1d10b6b24af8b866 size: 3479\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2020-12-10 23:27:11.088945: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:27:11.088998: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "WARNING:tensorflow:From /home/jupyter/imported/my_pipeline/pipeline/pipeline.py:75: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:absl:The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:Only one of `artifacts` and `matching_channel_name` should be set.\n",
      "INFO:absl:Adding upstream dependencies for component CsvExampleGen\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/imported/my_pipeline/my_pipeline.tar.gz\n",
      "Pipeline \"my_pipeline\" does not exist.\n",
      "\u001b[0m2020-12-10 23:27:16.116787: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2020-12-10 23:27:16.116845: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating a run for pipeline: my_pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Pipeline \"my_pipeline\" does not exist.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BkoIMUfj0ol2"
   },
   "source": [
    "You can find your training jobs in [Cloud AI Platform Jobs](https://console.cloud.google.com/ai-platform/jobs). If your pipeline completed successfully, you can find your model in [Cloud AI Platform Models](https://console.cloud.google.com/ai-platform/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4DRTFdTy0ol3"
   },
   "source": [
    "## Step 10. Ingest YOUR data to the pipeline\n",
    "\n",
    "We made a pipeline for a model using the Chicago Taxi dataset. Now it's time to put your data into the pipeline.\n",
    "\n",
    "Your data can be stored anywhere your pipeline can access, including GCS, or BigQuery. You will need to modify the pipeline definition to access your data.\n",
    "\n",
    "1. If your data is stored in files, modify the `DATA_PATH` in `kubeflow_dag_runner.py` or `beam_dag_runner.py` and set it to the location of your files. If your data is stored in BigQuery, modify `BIG_QUERY_QUERY` in `pipeline/configs.py` to correctly query for your data.\n",
    "1. Add features in `models/features.py`.\n",
    "1. Modify `models/preprocessing.py` to [transform input data for training](https://www.tensorflow.org/tfx/guide/transform).\n",
    "1. Modify `models/keras/model.py` and `models/keras/constants.py` to [describe your ML model](https://www.tensorflow.org/tfx/guide/trainer).\n",
    "  - You can use an estimator based model, too. Change `RUN_FN` constant to `models.estimator.model.run_fn` in `pipeline/configs.py`.\n",
    "\n",
    "Please see [Trainer component guide](https://www.tensorflow.org/tfx/guide/trainer) for more introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20KRGsPX0ol3"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Alternatively, you can clean up individual resources by visiting each consoles:\n",
    "- [Google Cloud Storage](https://console.cloud.google.com/storage)\n",
    "- [Google Container Registry](https://console.cloud.google.com/gcr)\n",
    "- [Google Kubernetes Engine](https://console.cloud.google.com/kubernetes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-1.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
