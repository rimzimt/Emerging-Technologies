{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Semantic Similarity.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO5fRgqHCAS+7D2LqkLUyzw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a500fb2e4b6c4127b9e48c6095f86096":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ef6da05c2ae14a5a988f35f888357bf9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fefd74c9d01145a1993e471b816bc83f","IPY_MODEL_8eb619f65d464effa41866de101fb128"]}},"ef6da05c2ae14a5a988f35f888357bf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fefd74c9d01145a1993e471b816bc83f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0ad800f080864d78acd91d4801f6cd18","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_31fc8f40e6364bb0b6bc3f97af639e52"}},"8eb619f65d464effa41866de101fb128":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_27596780e8ae4a06a4285cdbca54066c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 628kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_138aa209ed35487e93679afac5d0d36f"}},"0ad800f080864d78acd91d4801f6cd18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"31fc8f40e6364bb0b6bc3f97af639e52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"27596780e8ae4a06a4285cdbca54066c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"138aa209ed35487e93679afac5d0d36f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UP732r3lEqYc","executionInfo":{"status":"ok","timestamp":1607761840428,"user_tz":480,"elapsed":6977,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"a55ff07e-a6ae-46ea-8f3a-bc6a06e3ff56"},"source":[" pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n","\u001b[K     |████████████████████████████████| 1.4MB 11.6MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 54.3MB/s \n","\u001b[?25hCollecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 55.5MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=860f4db07fdc16fcf33ab08fa75feaf1b1cfc1e00daf96451faf81bb587f88d3\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r6cKukyVEY-O","executionInfo":{"status":"ok","timestamp":1607761876326,"user_tz":480,"elapsed":3652,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}}},"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import transformers"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"QtwJU1LAEkqu","executionInfo":{"status":"ok","timestamp":1607761940282,"user_tz":480,"elapsed":708,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}}},"source":["# configuration \n","\n","max_length = 128  \n","batch_size = 32\n","epochs = 2\n","\n","# labels\n","\n","labels = [\"contradiction\", \"entailment\", \"neutral\"]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MYpNe6N7FLQD","executionInfo":{"status":"ok","timestamp":1607761979747,"user_tz":480,"elapsed":1983,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"b6ba64ca-1003-4d87-91c6-aba6b04e7198"},"source":["# load the dataset \n","\n","!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n","!tar -xvzf data.tar.gz\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 11.1M  100 11.1M    0     0  11.8M      0 --:--:-- --:--:-- --:--:-- 11.8M\n","SNLI_Corpus/\n","SNLI_Corpus/snli_1.0_dev.csv\n","SNLI_Corpus/snli_1.0_train.csv\n","SNLI_Corpus/snli_1.0_test.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JF2wWqznFOF6","executionInfo":{"status":"ok","timestamp":1607761999943,"user_tz":480,"elapsed":612,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"87d37dc7-d3b3-4c5b-dc9f-eddab4191b7d"},"source":["# create training, test and validation dataset\n","train_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_train.csv\", nrows=100000)\n","valid_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_dev.csv\")\n","test_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_test.csv\")\n","\n","# Shape of the data\n","print(f\"Total train samples : {train_df.shape[0]}\")\n","print(f\"Total validation samples: {valid_df.shape[0]}\")\n","print(f\"Total test samples: {valid_df.shape[0]}\")\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Total train samples : 100000\n","Total validation samples: 10000\n","Total test samples: 10000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXpfEwqcFTRr","executionInfo":{"status":"ok","timestamp":1607762015577,"user_tz":480,"elapsed":607,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"87064253-28df-496b-b230-78a91f587276"},"source":["# check one example\n","print(f\"Sentence1: {train_df.loc[1, 'sentence1']}\")\n","print(f\"Sentence2: {train_df.loc[1, 'sentence2']}\")\n","print(f\"Similarity: {train_df.loc[1, 'similarity']}\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Sentence1: A person on a horse jumps over a broken down airplane.\n","Sentence2: A person is at a diner, ordering an omelette.\n","Similarity: contradiction\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smX_xVcVFXGv","executionInfo":{"status":"ok","timestamp":1607762069372,"user_tz":480,"elapsed":934,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"673da2db-c7dc-4216-b867-516070956e6a"},"source":["# preprocess the dataset \n","print(\"Number of missing values\")\n","print(train_df.isnull().sum())\n","train_df.dropna(axis=0, inplace=True)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Number of missing values\n","similarity    0\n","sentence1     0\n","sentence2     3\n","dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Ryz2BxRFkOs","executionInfo":{"status":"ok","timestamp":1607762088521,"user_tz":480,"elapsed":604,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"7d3fe038-395c-4b35-90fd-f44a96604352"},"source":["# check the distribution of datasets\n","print(\"Train Target Distribution\")\n","print(train_df.similarity.value_counts())"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Train Target Distribution\n","entailment       33384\n","contradiction    33310\n","neutral          33193\n","-                  110\n","Name: similarity, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"soSP9-ZHFo6g","executionInfo":{"status":"ok","timestamp":1607762115331,"user_tz":480,"elapsed":537,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"edd0ada1-a804-4fb6-f0a4-3719464dd11f"},"source":["# check distribution of validation dataset \n","print(\"Validation Target Distribution\")\n","print(valid_df.similarity.value_counts())"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Validation Target Distribution\n","entailment       3329\n","contradiction    3278\n","neutral          3235\n","-                 158\n","Name: similarity, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nK5guElWFvc-","executionInfo":{"status":"ok","timestamp":1607762134759,"user_tz":480,"elapsed":952,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}}},"source":["# skip examples with '-'\n","train_df = (\n","    train_df[train_df.similarity != \"-\"]\n","    .sample(frac=1.0, random_state=42)\n","    .reset_index(drop=True)\n",")\n","valid_df = (\n","    valid_df[valid_df.similarity != \"-\"]\n","    .sample(frac=1.0, random_state=42)\n","    .reset_index(drop=True)\n",")"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_PCXR7YF0MG","executionInfo":{"status":"ok","timestamp":1607762159615,"user_tz":480,"elapsed":575,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}}},"source":["# one hot encoding of columns\n","train_df[\"label\"] = train_df[\"similarity\"].apply(\n","    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",")\n","y_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n","\n","valid_df[\"label\"] = valid_df[\"similarity\"].apply(\n","    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",")\n","y_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n","\n","test_df[\"label\"] = test_df[\"similarity\"].apply(\n","    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",")\n","y_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)\n","\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"sM2zli7-Ker6","executionInfo":{"status":"ok","timestamp":1607763359271,"user_tz":480,"elapsed":693,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}}},"source":["class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n","    \"\"\"Generates batches of data.\n","    Args:\n","        sentence_pairs: Array of premise and hypothesis input sentences.\n","        labels: Array of labels.\n","        batch_size: Integer batch size.\n","        shuffle: boolean, whether to shuffle the data.\n","        include_targets: boolean, whether to incude the labels.\n","    Returns:\n","        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n","        (or just `[input_ids, attention_mask, `token_type_ids]`\n","         if `include_targets=False`)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        sentence_pairs,\n","        labels,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        include_targets=True,\n","    ):\n","        self.sentence_pairs = sentence_pairs\n","        self.labels = labels\n","        self.shuffle = shuffle\n","        self.batch_size = batch_size\n","        self.include_targets = include_targets\n","        # Load our BERT Tokenizer to encode the text.\n","        # We will use base-base-uncased pretrained model.\n","        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n","            \"bert-base-uncased\", do_lower_case=True\n","        )\n","        self.indexes = np.arange(len(self.sentence_pairs))\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        # Denotes the number of batches per epoch.\n","        return len(self.sentence_pairs) // self.batch_size\n","\n","    def __getitem__(self, idx):\n","        # Retrieves the batch of index.\n","        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n","        sentence_pairs = self.sentence_pairs[indexes]\n","\n","        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n","        # encoded together and separated by [SEP] token.\n","        encoded = self.tokenizer.batch_encode_plus(\n","            sentence_pairs.tolist(),\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            return_attention_mask=True,\n","            return_token_type_ids=True,\n","            pad_to_max_length=True,\n","            return_tensors=\"tf\",\n","        )\n","\n","        # Convert batch of encoded features to numpy array.\n","        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n","        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n","        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n","\n","        # Set to true if data generator is used for training/validation.\n","        if self.include_targets:\n","            labels = np.array(self.labels[indexes], dtype=\"int32\")\n","            return [input_ids, attention_masks, token_type_ids], labels\n","        else:\n","            return [input_ids, attention_masks, token_type_ids]\n","\n","    def on_epoch_end(self):\n","        # Shuffle indexes after each epoch if shuffle is set to True.\n","        if self.shuffle:\n","            np.random.RandomState(42).shuffle(self.indexes)\n"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZq3tkhOHGMP","executionInfo":{"status":"ok","timestamp":1607763461408,"user_tz":480,"elapsed":4904,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"f24dea09-b007-4c51-eb6a-e1f5ba865313"},"source":["# Create the model under a distribution strategy scope.\n","strategy = tf.distribute.MirroredStrategy()\n","\n","with strategy.scope():\n","    # Encoded token ids from BERT tokenizer.\n","    input_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n","    )\n","    # Attention masks indicates to the model which tokens should be attended to.\n","    attention_masks = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n","    )\n","    # Token type ids are binary masks identifying different sequences in the model.\n","    token_type_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n","    )\n","    # Loading pretrained BERT model.\n","    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n","    # Freeze the BERT model to reuse the pretrained features without modifying them.\n","    bert_model.trainable = False\n","\n","    sequence_output, pooled_output = bert_model(\n","        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids,return_dict=False\n","    )\n","    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n","    bi_lstm = tf.keras.layers.Bidirectional(\n","        tf.keras.layers.LSTM(64, return_sequences=True)\n","    )(sequence_output)\n","    # Applying hybrid pooling approach to bi_lstm sequence output.\n","    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n","    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n","    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n","    dropout = tf.keras.layers.Dropout(0.3)(concat)\n","    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n","    model = tf.keras.models.Model(\n","        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n","    )\n","\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(),\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"acc\"],\n","    )"],"execution_count":20,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"],"name":"stdout"},{"output_type":"stream","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jhv7w6XBK9LL","executionInfo":{"status":"ok","timestamp":1607763483550,"user_tz":480,"elapsed":631,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"b46ee881-11e4-4364-e27d-e219190e0f00"},"source":["print(f\"Strategy: {strategy}\")\n","model.summary()"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7ff241ea5128>\n","Model: \"functional_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_ids (InputLayer)          [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","attention_masks (InputLayer)    [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","token_type_ids (InputLayer)     [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","tf_bert_model_4 (TFBertModel)   ((None, 128, 768), ( 109482240   input_ids[0][0]                  \n","                                                                 attention_masks[0][0]            \n","                                                                 token_type_ids[0][0]             \n","__________________________________________________________________________________________________\n","bidirectional_4 (Bidirectional) (None, 128, 128)     426496      tf_bert_model_4[0][0]            \n","__________________________________________________________________________________________________\n","global_average_pooling1d (Globa (None, 128)          0           bidirectional_4[0][0]            \n","__________________________________________________________________________________________________\n","global_max_pooling1d (GlobalMax (None, 128)          0           bidirectional_4[0][0]            \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n","                                                                 global_max_pooling1d[0][0]       \n","__________________________________________________________________________________________________\n","dropout_185 (Dropout)           (None, 256)          0           concatenate[0][0]                \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 3)            771         dropout_185[0][0]                \n","==================================================================================================\n","Total params: 109,909,507\n","Trainable params: 427,267\n","Non-trainable params: 109,482,240\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["a500fb2e4b6c4127b9e48c6095f86096","ef6da05c2ae14a5a988f35f888357bf9","fefd74c9d01145a1993e471b816bc83f","8eb619f65d464effa41866de101fb128","0ad800f080864d78acd91d4801f6cd18","31fc8f40e6364bb0b6bc3f97af639e52","27596780e8ae4a06a4285cdbca54066c","138aa209ed35487e93679afac5d0d36f"]},"id":"0kPd9PJxHfS1","executionInfo":{"status":"ok","timestamp":1607763496896,"user_tz":480,"elapsed":2449,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"cfa77b86-b4f1-4994-f055-db9319861dd6"},"source":["# create training and validation data generators \n","train_data = BertSemanticDataGenerator(\n","    train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n","    y_train,\n","    batch_size=batch_size,\n","    shuffle=True,\n",")\n","valid_data = BertSemanticDataGenerator(\n","    valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n","    y_val,\n","    batch_size=batch_size,\n","    shuffle=False,\n",")"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a500fb2e4b6c4127b9e48c6095f86096","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dn-P0lUoLDWa","executionInfo":{"status":"ok","timestamp":1607764385353,"user_tz":480,"elapsed":868705,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"21d9c0ca-d475-48e2-92c0-30292d929b77"},"source":["# train the model\n","history = model.fit(\n","    train_data,\n","    validation_data=valid_data,\n","    epochs=epochs,\n","    use_multiprocessing=True,\n","    workers=-1,\n",")"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/2\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Iterator.get_next_as_optional()` instead.\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","3121/3121 [==============================] - ETA: 0s - loss: 0.6938 - acc: 0.7051"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","3121/3121 [==============================] - 427s 137ms/step - loss: 0.6938 - acc: 0.7051 - val_loss: 0.5331 - val_acc: 0.7866\n","Epoch 2/2\n","3121/3121 [==============================] - 424s 136ms/step - loss: 0.5930 - acc: 0.7593 - val_loss: 0.5008 - val_acc: 0.8009\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0xLA8swLLFvr"},"source":["# fine tuning \n","bert_model.trainable = True\n","# Recompile the model to make the change effective.\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(1e-5),\n","    loss=\"categorical_crossentropy\",\n","    metrics=[\"accuracy\"],\n",")\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nqc7uQhrPni3","executionInfo":{"status":"ok","timestamp":1607765554817,"user_tz":480,"elapsed":850055,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"fb61db8b-02e3-4165-dec2-081fa1f0efc3"},"source":["history = model.fit(\n","    train_data,\n","    validation_data=valid_data,\n","    epochs=epochs,\n","    use_multiprocessing=True,\n","    workers=-1,\n",")"],"execution_count":24,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/2\n","3121/3121 [==============================] - 425s 136ms/step - loss: 0.5632 - acc: 0.7725 - val_loss: 0.4836 - val_acc: 0.8126\n","Epoch 2/2\n","3121/3121 [==============================] - 424s 136ms/step - loss: 0.5441 - acc: 0.7813 - val_loss: 0.4713 - val_acc: 0.8191\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x1mS3L8-S_NV","executionInfo":{"status":"ok","timestamp":1607765590391,"user_tz":480,"elapsed":606,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}}},"source":["def check_similarity(sentence1, sentence2):\n","    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n","    test_data = BertSemanticDataGenerator(\n","        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n","    )\n","\n","    proba = model.predict(test_data)[0]\n","    idx = np.argmax(proba)\n","    proba = f\"{proba[idx]: .2f}%\"\n","    pred = labels[idx]\n","    return pred, proba"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L17IIX_bTDhl","executionInfo":{"status":"ok","timestamp":1607765612321,"user_tz":480,"elapsed":5229,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"cf068bb5-9051-49e4-c79e-5f109881ffa5"},"source":["sentence1 = \"Two women are observing something together.\"\n","sentence2 = \"Two women are standing with their eyes closed.\"\n","check_similarity(sentence1, sentence2)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["('contradiction', ' 0.60%')"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZqzJWXQTR89","executionInfo":{"status":"ok","timestamp":1607765667424,"user_tz":480,"elapsed":866,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"db4b961a-ba82-4fe2-c5c9-eeb4afae5a7e"},"source":["sentence1 = \"A smiling costumed woman is holding an umbrella\"\n","sentence2 = \"A happy woman in a fairy costume holds an umbrella\"\n","check_similarity(sentence1, sentence2)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["('neutral', ' 0.78%')"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cCvNcGROTR6D","executionInfo":{"status":"ok","timestamp":1607765679720,"user_tz":480,"elapsed":1192,"user":{"displayName":"Sushrut Shirole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPZrnQIu9Fm7L-WhwTi4lFZL814qmLkOC7oiuJpQ=s64","userId":"11387990540572392766"}},"outputId":"50fc54c5-4a15-4a7b-d0e9-2c48d22c2cf9"},"source":["sentence1 = \"A soccer game with multiple males playing\"\n","sentence2 = \"Some men are playing a sport\"\n","check_similarity(sentence1, sentence2)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["('entailment', ' 0.94%')"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"7eOzO9MrTVmc"},"source":[""],"execution_count":null,"outputs":[]}]}