{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment 5.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1EBvz7pw01Z93agiz4rrVRYU29opSwqvX","authorship_tag":"ABX9TyN1E6qLvIxAB2OMl6q8Rfu8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0fgyH5OPI7PW","executionInfo":{"status":"ok","timestamp":1605671915080,"user_tz":480,"elapsed":412,"user":{"displayName":"Rimzim Thube","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLsluMvnDGN7UrXPCRHxQJCJ4691H4hD0GwCBw=s64","userId":"15821276931952451987"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8otNrw58T7LE","executionInfo":{"status":"ok","timestamp":1605671917478,"user_tz":480,"elapsed":2801,"user":{"displayName":"Rimzim Thube","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLsluMvnDGN7UrXPCRHxQJCJ4691H4hD0GwCBw=s64","userId":"15821276931952451987"}},"outputId":"4061558a-976c-4386-ff7f-4c8a9671f4ec"},"source":["!pip install torch torchvision\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.7)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eQNzi55MJHiy","executionInfo":{"status":"ok","timestamp":1605671917479,"user_tz":480,"elapsed":2797,"user":{"displayName":"Rimzim Thube","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLsluMvnDGN7UrXPCRHxQJCJ4691H4hD0GwCBw=s64","userId":"15821276931952451987"}}},"source":["# Implement multi head attention layer \n","class MultiHeadSelfAttention(layers.Layer):\n","    def __init__(self, embeddim, numheads=8):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.embeddim = embeddim  # embedded dimensions  \n","        self.numheads = numheads  # number of heads \n","        if embeddim % numheads != 0:\n","            raise ValueError(\n","                f\"embedding dimension should be divisible by no of heads\"\n","            )\n","        self.projectiondim = embeddim // numheads  # projected dimensions \n","        self.querydense = layers.Dense(embeddim)   # query layer\n","        self.keydense = layers.Dense(embeddim)     # key layer\n","        self.valuedense = layers.Dense(embeddim)   # value layer \n","        self.combineheads = layers.Dense(embeddim) # combination layer\n","\n","    # attention layer \n","    def attention(self, query, key, value):\n","        score = tf.matmul(query, key, transpose_b=True)\n","        dimkey = tf.cast(tf.shape(key)[-1], tf.float32)\n","        scaledscore = score / tf.math.sqrt(dimkey)\n","        weights = tf.nn.softmax(scaledscore, axis=-1)\n","        output = tf.matmul(weights, value)\n","        return output, weights\n","\n","    # create multiple heads \n","    def separateheads(self, x, batchsize):\n","        x = tf.reshape(x, (batchsize, -1, self.numheads, self.projectiondim))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","\n","    def call(self, inputs):\n","        batchsize = tf.shape(inputs)[0]   # define batch size \n","        query = self.querydense(inputs)   # create query layer \n","        key = self.keydense(inputs)       # create key layer \n","        value = self.valuedense(inputs)   # create value layer \n","        query = self.separateheads(\n","            query, batchsize\n","        )                                 # separate heads for query \n","        key = self.separateheads(\n","            key, batchsize\n","        )                                 # separate heads for key \n","        value = self.separateheads(\n","            value, batchsize\n","        )                                 # separate heads for value \n","        attention, weights = self.attention(query, key, value) # create the attention layer \n","        attention = tf.transpose(\n","            attention, perm=[0, 2, 1, 3]\n","        )  \n","        concatattention = tf.reshape(\n","            attention, (batchsize, -1, self.embeddim)\n","        )                                 # concate the attention layer \n","        output = self.combineheads(\n","            concatattention\n","        )                                 # create the output layer \n","        return output"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5ZBBH5qKyqf","executionInfo":{"status":"ok","timestamp":1605671917480,"user_tz":480,"elapsed":2794,"user":{"displayName":"Rimzim Thube","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLsluMvnDGN7UrXPCRHxQJCJ4691H4hD0GwCBw=s64","userId":"15821276931952451987"}}},"source":["class TransformerBlock(layers.Layer):\n","    def __init__(self, embeddim, numheads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = MultiHeadSelfAttention(embeddim, numheads)  # attention layer \n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embeddim),]\n","        )                                                     # feed forward layer \n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6) # layer normalization\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6) # layer normalization\n","        self.dropout1 = layers.Dropout(rate)                  # dropout layer \n","        self.dropout2 = layers.Dropout(rate)                  # dropout layer \n","\n","    # call the different layers \n","    def call(self, inputs, training):\n","        attnoutput = self.att(inputs)                        \n","        attnoutput = self.dropout1(attnoutput, training=training) \n","        out1 = self.layernorm1(inputs + attnoutput)\n","        ffnoutput = self.ffn(out1)\n","        ffnoutput = self.dropout2(ffnoutput, training=training)\n","        return self.layernorm2(out1 + ffnoutput)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"4T4hfpeKLBKo","executionInfo":{"status":"ok","timestamp":1605671917480,"user_tz":480,"elapsed":2790,"user":{"displayName":"Rimzim Thube","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLsluMvnDGN7UrXPCRHxQJCJ4691H4hD0GwCBw=s64","userId":"15821276931952451987"}}},"source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocabsize, embeddim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocabsize, output_dim=embeddim) # token embedding\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embeddim)      # position embedding \n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJicKFWeLGbq","executionInfo":{"status":"ok","timestamp":1605671923520,"user_tz":480,"elapsed":8827,"user":{"displayName":"Rimzim Thube","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLsluMvnDGN7UrXPCRHxQJCJ4691H4hD0GwCBw=s64","userId":"15821276931952451987"}},"outputId":"c55a298d-b1d3-4b77-a560-d65697d091ef"},"source":["vocabsize = 10000  # Consider top 10K words\n","maxlen = 250  # consider first 250 words of imdb reviews \n","(xtrain, ytrain), (xval, yval) = keras.datasets.imdb.load_data(num_words=vocabsize) # imdb dataset\n","print(len(xtrain), \"training\")\n","print(len(xval), \"validation\")\n","xtrain = keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=maxlen)\n","xval = keras.preprocessing.sequence.pad_sequences(xval, maxlen=maxlen)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["25000 training\n","25000 validation\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SZPzeZeyLJAj","executionInfo":{"status":"ok","timestamp":1605671923681,"user_tz":480,"elapsed":8983,"user":{"displayName":"Rimzim Thube","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLsluMvnDGN7UrXPCRHxQJCJ4691H4hD0GwCBw=s64","userId":"15821276931952451987"}}},"source":["embeddim = 32  # embedding for each word \n","numheads = 2  # attention heads \n","ff_dim = 32  # hidde layer size \n","\n","inputs = layers.Input(shape=(maxlen,))    # input layer \n","embedding_layer = TokenAndPositionEmbedding(maxlen, vocabsize, embeddim) # embedding layer \n","x = embedding_layer(inputs)\n","transformer_block = TransformerBlock(embeddim, numheads, ff_dim) # transformer \n","x = transformer_block(x)\n","x = layers.GlobalAveragePooling1D()(x)\n","x = layers.Dropout(0.1)(x)\n","x = layers.Dense(20, activation=\"relu\")(x)\n","x = layers.Dropout(0.1)(x)\n","outputs = layers.Dense(2, activation=\"softmax\")(x)\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPajkYC5LKv_","executionInfo":{"status":"ok","timestamp":1605671955372,"user_tz":480,"elapsed":40670,"user":{"displayName":"Rimzim Thube","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLsluMvnDGN7UrXPCRHxQJCJ4691H4hD0GwCBw=s64","userId":"15821276931952451987"}},"outputId":"3c623c56-eb26-468b-842d-7f24589e7b97"},"source":["# compile and fit the model\n","model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","history = model.fit(\n","    xtrain, ytrain, batch_size=32, epochs=2, validation_data=(xval, yval)\n",")\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n","782/782 [==============================] - 15s 19ms/step - loss: 0.3842 - accuracy: 0.8189 - val_loss: 0.2915 - val_accuracy: 0.8757\n","Epoch 2/2\n","782/782 [==============================] - 14s 18ms/step - loss: 0.2160 - accuracy: 0.9158 - val_loss: 0.2906 - val_accuracy: 0.8829\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"di0Pp0cKcEDm","executionInfo":{"status":"ok","timestamp":1605671955373,"user_tz":480,"elapsed":40666,"user":{"displayName":"Rimzim Thube","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLsluMvnDGN7UrXPCRHxQJCJ4691H4hD0GwCBw=s64","userId":"15821276931952451987"}}},"source":[""],"execution_count":17,"outputs":[]}]}