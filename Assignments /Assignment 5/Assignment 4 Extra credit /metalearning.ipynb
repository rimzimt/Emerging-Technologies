{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metaSGD_metalearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zWLF2VvorOm"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgPJb3QMqVQO"
      },
      "source": [
        "def sample_points(k):\n",
        "    x = np.random.rand(k,50)\n",
        "    y = np.random.choice([0, 1], size=k, p=[.5, .5]).reshape([-1,1])\n",
        "    return x,y"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ftnowh0qZZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac659776-6048-4550-b865-c14b087fe3a8"
      },
      "source": [
        "x, y = sample_points(10)\n",
        "print(x[0])\n",
        "print(y[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.50085413 0.24295302 0.14185116 0.35753217 0.40088321 0.68094062\n",
            " 0.65014807 0.47250302 0.52174673 0.37652713 0.96161274 0.7184758\n",
            " 0.72663163 0.68449836 0.01649995 0.87995769 0.12940246 0.81647012\n",
            " 0.24712716 0.54201573 0.76321204 0.85836369 0.83265051 0.61398207\n",
            " 0.33077389 0.11346194 0.63686537 0.20201624 0.52844823 0.55186246\n",
            " 0.31077917 0.91658018 0.44616741 0.69064677 0.91723487 0.36138512\n",
            " 0.1527355  0.4939028  0.85421846 0.9297398  0.0109952  0.32573835\n",
            " 0.62444273 0.95456336 0.32196629 0.54428154 0.13691608 0.38649084\n",
            " 0.52037837 0.3293441 ]\n",
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJnR7u-Eqg2Q"
      },
      "source": [
        "class MetaSGD(object):\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.num_tasks = 2\n",
        "        \n",
        "        self.num_samples = 10\n",
        "\n",
        "        self.epochs = 10000\n",
        "        \n",
        "        self.alpha = 0.0001\n",
        "        \n",
        "        self.beta = 0.0001\n",
        "       \n",
        "        # theta value\n",
        "        self.theta = np.random.normal(size=50).reshape(50, 1)\n",
        "         \n",
        "        # alpha value\n",
        "        self.alpha = np.random.normal(size=50).reshape(50, 1)\n",
        "      \n",
        "    # sigmoid function \n",
        "    def sigmoid(self,a):\n",
        "        return 1.0 / (1 + np.exp(-a))\n",
        "    \n",
        "    \n",
        "    # training part \n",
        "    def train(self):\n",
        "        \n",
        "        for e in range(self.epochs):        \n",
        "            \n",
        "            self.theta_ = []\n",
        "            \n",
        "            for i in range(self.num_tasks):\n",
        "               \n",
        "                # training data set \n",
        "                XTrain, YTrain = sample_points(self.num_samples)\n",
        "                \n",
        "                a = np.matmul(XTrain, self.theta)\n",
        "\n",
        "                YHat = self.sigmoid(a)\n",
        "\n",
        "                loss = ((np.matmul(-YTrain.T, np.log(YHat)) - np.matmul((1 -YTrain.T), np.log(1 - YHat)))/self.num_samples)[0][0]\n",
        "                \n",
        "                # gradient values \n",
        "                gradient = np.matmul(XTrain.T, (YHat - YTrain)) / self.num_samples\n",
        "\n",
        "                # update theta values \n",
        "                self.theta_.append(self.theta - (np.multiply(self.alpha,gradient)))\n",
        "                \n",
        "     \n",
        "            # meta gradient values \n",
        "            meta_gradient = np.zeros(self.theta.shape)\n",
        "                        \n",
        "            for i in range(self.num_tasks):\n",
        "            \n",
        "                # x test and y test values \n",
        "                XTest, YTest = sample_points(10)\n",
        "\n",
        "                a = np.matmul(XTest, self.theta_[i])\n",
        "                \n",
        "                YPred = self.sigmoid(a)\n",
        "                           \n",
        "                # meta gradients\n",
        "                meta_gradient += np.matmul(XTest.T, (YPred - YTest)) / self.num_samples\n",
        "\n",
        "            #update  theta with the meta gradients\n",
        "            self.theta = self.theta-self.beta*meta_gradient/self.num_tasks\n",
        "                       \n",
        "            # update the alpha with the meta gradients\n",
        "            self.alpha = self.alpha-self.beta*meta_gradient/self.num_tasks\n",
        "                                       \n",
        "            if e%1000==0:\n",
        "                print(\"Epoch {}: Loss {}\\n\".format(e,loss))           \n",
        "                print('Updated Model Parameter Theta\\n')\n",
        "                print('Sampling Next Batch of Tasks \\n')\n",
        "                print('---------------------------------\\n')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puBlp30gqsQe"
      },
      "source": [
        "model = MetaSGD()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs_xGUP5qvvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c5cc550-ca6c-4c48-82fd-11c930bfc31d"
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss 2.220636675285848\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 1000: Loss 1.9629621097748235\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 2000: Loss 1.1748940254869602\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 3000: Loss 1.8254859897969087\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 4000: Loss 1.183682834312381\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 5000: Loss 2.2491124246129806\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 6000: Loss 0.2336043698059756\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 7000: Loss 1.9874010164246105\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 8000: Loss 1.9384608941842043\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 9000: Loss 1.2478095298613523\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}