{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.11"
    },
    "colab": {
      "name": "3.3 Omniglot Character set classification using Prototypical Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV8v4lQaknCO"
      },
      "source": [
        "# Omniglot Character Set Classification Using Prototypical Network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTZ6WxCWknCQ",
        "outputId": "752fe4a3-1189-4a2e-c0e9-3ca4541b3b13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "!pip install tensorflow==1.14\n",
        "\n",
        "import tensorflow as tf\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/59/d88fe8c58ffb66aca21d03c0e290cd68327cc133591130c674985e98a482/tensorflow-1.14.0-cp27-cp27mu-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (3.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (0.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (0.35.1)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (1.1.6)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (0.7.1)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/37/e6a7af1c92c5b68fb427f853b06164b56ea92126bcfd87784334ec5e4d42/tensorboard-1.14.0-py2-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 53.8MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (0.1.7)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (1.0.post1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.14) (0.8.0)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==1.14) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.14) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.14) (5.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (44.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.1.1)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.1.0\n",
            "    Uninstalling tensorboard-2.1.0:\n",
            "      Successfully uninstalled tensorboard-2.1.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.0\n",
            "    Uninstalling tensorflow-estimator-1.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.0\n",
            "  Found existing installation: tensorflow 2.1.0\n",
            "    Uninstalling tensorflow-2.1.0:\n",
            "      Successfully uninstalled tensorflow-2.1.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36NL_Yc6knCV",
        "outputId": "889dbbf3-7362-4e32-b9fe-2132f6c1658f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# view sample images \n",
        "Image.open('/content/drive/My Drive/Emerging Technologies/Assignment 4/data/images/Japanese_(katakana)/character13/0608_01.png')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAABpAQAAAAAR+TCXAAAAqUlEQVR4nO3OMQ6CQBCF4X9wEyi3tOQYlhQeywKiByOehNJSuzUhjIXE8DDGimjBVPvlzeyMOZNqMqRWvvP0+zM8wqBpdTOz4kWPQA8EuJt8lb4uyt19wgQegIDhnCfNx+drGPcexiTKz70uSqXwqmlXCdta2GhzJvQgHAphj5AovJTCrhK2s9lauNF0r9zNZv+ebmYWNU2fmkvlFqAAAgC5S7rczSuX5gMYvSMsiQG6eAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=1 size=105x105 at 0x7F93E033A810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGC4BoNKohb6",
        "outputId": "1b96d7ae-eac8-461a-c0db-91f6c59730c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yf5dCoHknCY"
      },
      "source": [
        "the same alphabet in different variation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xpJ_BFLCknCZ",
        "outputId": "eb786f88-413c-4075-cd82-485951b2b742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# view sample images \n",
        "\n",
        "Image.open('/content/drive/My Drive/Emerging Technologies/Assignment 4/data/images/Japanese_(katakana)/character13/0608_13.png')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAABpAQAAAAAR+TCXAAAAy0lEQVR4nO2SsQ3CQBAE594ICAhcAIFLoAKgNJsKKIFSEJVABwghYZDxEfhtWCMRIgIuG+3e/t3/m/NSRUDqj294+I0x9ooG3NR8UTwuBTeiei7qiTziGLin1por4BxtAWoo0tgzALf3mUcdbgErBy3OgKF36gQW5TMq8d2ceixRVaob6UFkgqWq66VgLqpHblUTtQ6CYarJmaBte1P9ALpi1jPnn3rTnnkFVwCad3SL8QGaz9L9nFhPHGqUVe1FN1GJv6qfN/rjt/EBTzonExbNmAgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=1 size=105x105 at 0x7F93BF63FE90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA0LJYsUknCk"
      },
      "source": [
        "imagename = '/content/drive/My Drive/Emerging Technologies/Assignment 4/data/images/Sanskrit/character13/0863_13.png'\n",
        "alphabet, character, rotation = 'Sanskrit/character13/rot000'.split('/')\n",
        "rotation = float(rotation[3:])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "BQCmCG6RknCm",
        "outputId": "76041e4c-f6b6-4f57-eaee-4dceb244404a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# convert the images in vector format \n",
        "\n",
        "np.array(Image.open(imagename).rotate(rotation).resize((28, 28)), np.float32,copy=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLumlBN-knCp"
      },
      "source": [
        "rootdir = '/content/drive/My Drive/Emerging Technologies/Assignment 4/data/'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-Qnn9N2knCr"
      },
      "source": [
        "We have the splitting details in the /data/omniglot/splits/train.txt file which has the language name, character number, rotation information and images in /data/omniglot/data/ directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7B22AjyknCr"
      },
      "source": [
        "# split the trainind data details \n",
        "trainsplitpath = os.path.join(rootdir, 'splits', 'train.txt')\n",
        "\n",
        "with open(trainsplitpath, 'r') as train_split:\n",
        "    trainclasses = [line.rstrip() for line in train_split.readlines()]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7_GMRTZknCt"
      },
      "source": [
        "# store the number of classes \n",
        "noofclasses = len(trainclasses)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgBZEgDIknCw"
      },
      "source": [
        "Now we set the number of examples to 20, as we have 20 example per class in our dataset, and also we set image width and height to 28 x 28:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4mTwyOEknCx"
      },
      "source": [
        "# specify the number of examples, width and height \n",
        "numexamples = 20\n",
        "\n",
        "imgwidth = 28\n",
        "\n",
        "imgheight = 28\n",
        "channels = 1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHJzPCMEknCz"
      },
      "source": [
        "Next, we initialize our training dataset with a shape as a number of classes, number of examples, image height and image width:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA7laU-cknC0"
      },
      "source": [
        "# load the training dataset \n",
        "traindataset = np.zeros([noofclasses, numexamples, imgheight, imgwidth], dtype=np.float32)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lSprPmiknC3"
      },
      "source": [
        "Now, we read all the images, convert it to numpy array and store it our traindataset array with their label and values, that is,  traindataset = [label, values]:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYGqaJLmknC4"
      },
      "source": [
        "# read the images, convert into array and store with label and values \n",
        "for label, name in enumerate(trainclasses):\n",
        "    alphabet, character, rotation = name.split('/')\n",
        "    rotation = float(rotation[3:])\n",
        "    img_dir = os.path.join(rootdir, 'data', alphabet, character)\n",
        "    img_files = sorted(glob.glob(os.path.join(img_dir, '*.png')))\n",
        "  \n",
        "    \n",
        "    for index, img_file in enumerate(img_files):\n",
        "        values = 1. - np.array(Image.open(img_file).rotate(rotation).resize((imgwidth, imgheight)), np.float32, copy=False)\n",
        "        traindataset[label, index] = values"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNFyfL4LknC6",
        "outputId": "1c4dbe39-3b0f-4c93-820e-82c34d2e36ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "traindataset.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4112, 20, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UcwqzYGknC8"
      },
      "source": [
        "Now that we have loaded our training data, we need to create embeddings for them. We generate the embeddings using a convolution operation as our inputs are images. So, we define a convolutional block with 64 filters with batch normalization and ReLU as the activation function. Followed by we perform max pooling operation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KU8_i7OknC8"
      },
      "source": [
        "# define convolution block \n",
        "def convolutionblock(inputs, out_channels, name='conv'):\n",
        "\n",
        "    conv = tf.layers.conv2d(inputs, out_channels, kernel_size=3, padding='SAME')\n",
        "    conv = tf.contrib.layers.batch_norm(conv, updates_collections=None, decay=0.99, scale=True, center=True)\n",
        "    conv = tf.nn.relu(conv)\n",
        "    conv = tf.contrib.layers.max_pool2d(conv, 2)\n",
        "    \n",
        "    return conv"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb_bRqF8knC-"
      },
      "source": [
        "Now, we define our embedding function which gives us the embedding comprising of four convolutional blocks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixN8Ds32knC_"
      },
      "source": [
        "# generate embeddings using convolution operation \n",
        "def getembeddings(support_set, h_dim, z_dim, reuse=False):\n",
        "\n",
        "        net = convolutionblock(support_set, h_dim)\n",
        "        net = convolutionblock(net, h_dim)\n",
        "        net = convolutionblock(net, h_dim) \n",
        "        net = convolutionblock(net, z_dim) \n",
        "        net = tf.contrib.layers.flatten(net)\n",
        "        \n",
        "        return net"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKaC0NgNknDB"
      },
      "source": [
        "Remember, we don't use our whole dataset for training, since we are one shot learning, we sample some data points from each class as a support set and train the network using the support set in an episodic fashion. \n",
        "\n",
        "\n",
        "Now we define some of the important variables, we consider a 60-way 5-shot learning scenario:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEXRVCaWknDB"
      },
      "source": [
        "# classes \n",
        "numway = 60  \n",
        "\n",
        "# examples per class \n",
        "num_shot = 5  \n",
        "\n",
        "# query points \n",
        "num_query = 5 \n",
        "\n",
        "# examples \n",
        "numexamples = 20\n",
        "\n",
        "h_dim = 64\n",
        "\n",
        "z_dim = 64"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xD1ySsGknDD"
      },
      "source": [
        "Next, we initialize placeholders for our support set and query set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2r4WuPqknDE"
      },
      "source": [
        "# define support and query set \n",
        "support_set = tf.placeholder(tf.float32, [None, None, imgheight, imgwidth, channels])\n",
        "query_set = tf.placeholder(tf.float32, [None, None, imgheight, imgwidth, channels])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uMGu2bQzXZX",
        "outputId": "7d95a176-09f5-43ca-d5c2-68a3ebb9d56d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB6EyGZTknDG"
      },
      "source": [
        "And we store the shape of our support set and query set in support_set_shape and query_set_shape respectively:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGDHYhDSknDG"
      },
      "source": [
        "support_set_shape = tf.shape(support_set)\n",
        "query_set_shape = tf.shape(query_set)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCfhCDkzknDI"
      },
      "source": [
        "Get the number of classes and number of data points in the support set and number of data points in the query set for initializing our support and query sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A35FEU7UknDI"
      },
      "source": [
        "# get number of classes and data points \n",
        "num_classes, num_support_points = support_set_shape[0], support_set_shape[1]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWf9U_WWknDK"
      },
      "source": [
        "num_query_points = query_set_shape[1]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TxbIVOOknDN"
      },
      "source": [
        "Next, we define the placeholder for our label:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4dMD15dknDN"
      },
      "source": [
        "y = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "# one hot encoding \n",
        "yonehot = tf.one_hot(y, depth=num_classes)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CvvDi6eknDP"
      },
      "source": [
        "Now, we generate the embeddings for our support set using our embedding function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVIwSwLMknDQ",
        "outputId": "47f238d1-f5bd-4207-c9e8-9d8d47d53356",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# generate embeddings \n",
        "\n",
        "supportsetembeddings = getembeddings(tf.reshape(support_set, [num_classes * num_support_points, imgheight, imgwidth, channels]), h_dim, z_dim)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W1031 14:15:00.131005 140273433495424 deprecation.py:323] From <ipython-input-15-4ba6064d20c1>:3: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "W1031 14:15:00.137475 140273433495424 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W1031 14:15:01.364614 140273433495424 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W1031 14:15:01.616895 140273433495424 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2HlHBhRknDS"
      },
      "source": [
        "We compute the prototype of each class which is the mean vector of the support set embeddings of the class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVaSG5DVknDT"
      },
      "source": [
        "# compute prototpye of each class \n",
        "embeddingdimension = tf.shape(supportsetembeddings)[-1]\n",
        "\n",
        "classprototype = tf.reduce_mean(tf.reshape(supportsetembeddings, [num_classes, num_support_points, embeddingdimension]), axis=1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpFeV5aIknDU"
      },
      "source": [
        "Next, we use our same embedding function for getting embeddings of the query set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26K8DyIbknDV"
      },
      "source": [
        "# query set embeddings \n",
        "querysetembeddings = getembeddings(tf.reshape(query_set, [num_classes * num_query_points, imgheight, imgwidth, channels]), h_dim, z_dim, reuse=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR5thjXOknDY"
      },
      "source": [
        "Now that, we have the class prototype and query set embeddings, we define a distance function which gives us the distance between the class prototypes and query set embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2wwq7JQknDY"
      },
      "source": [
        "# define euclidean distance \n",
        "def euclideandistance(a, b):\n",
        "\n",
        "    N, D = tf.shape(a)[0], tf.shape(a)[1]\n",
        "    M = tf.shape(b)[0]\n",
        "    a = tf.tile(tf.expand_dims(a, axis=1), (1, M, 1))\n",
        "    b = tf.tile(tf.expand_dims(b, axis=0), (N, 1, 1))\n",
        "    return tf.reduce_mean(tf.square(a - b), axis=2)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7pxxzE9knDa"
      },
      "source": [
        "Calculate the distance between the class prototype and query set embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89P0LFweknDb"
      },
      "source": [
        "distance = euclideandistance(classprototype,querysetembeddings)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRvZb1nkknDc"
      },
      "source": [
        "Next, we get the probability for each class as a softmax to the distance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ5YW6F2knDd"
      },
      "source": [
        "# probability of each class \n",
        "predicted_probability = tf.reshape(tf.nn.log_softmax(-distance), [num_classes, num_query_points, -1])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTJzjJhmknDf"
      },
      "source": [
        "Compute the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxv4q_92knDg"
      },
      "source": [
        "# compute loss \n",
        "loss = -tf.reduce_mean(tf.reshape(tf.reduce_sum(tf.multiply(yonehot, predicted_probability), axis=-1), [-1]))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLCb7CO9knDi"
      },
      "source": [
        "Calculate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1db5DXcknDj",
        "outputId": "b279a321-23b7-490a-e1a6-5fe5e3538d7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# accuracy \n",
        "accuracy = tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(predicted_probability, axis=-1), y)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1031 14:15:10.746479 140273433495424 deprecation.py:323] From <ipython-input-31-8b342a457945>:1: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8SjLGcSknDl"
      },
      "source": [
        "We use Adam optimizer for minimizing the loss:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-k6LmcrknDm"
      },
      "source": [
        "# use Adam optimizer to reduce loss\n",
        "train = tf.train.AdamOptimizer().minimize(loss)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsfoZ1rcknDo"
      },
      "source": [
        "Now, we start our tensorflow session and train the model,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93CHiBxqknDo"
      },
      "source": [
        "# start tensorflow session \n",
        "sess = tf.InteractiveSession()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17ZAsESFknDq"
      },
      "source": [
        "# defien epochs and episodes \n",
        "numepochs = 20\n",
        "numepisodes = 100"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "SxgdOf5dknDt",
        "outputId": "1e135b1c-d8b4-49e1-a162-cbff74b40265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for epoch in range(numepochs):\n",
        "    \n",
        "    for episode in range(numepisodes):\n",
        "        \n",
        "        # select 60 classes\n",
        "        episodicclasses = np.random.permutation(noofclasses)[:numway]\n",
        "        \n",
        "        support = np.zeros([numway, num_shot, imgheight, imgwidth], dtype=np.float32)\n",
        "        \n",
        "        query = np.zeros([numway, num_query, imgheight, imgwidth], dtype=np.float32)\n",
        "        \n",
        "        \n",
        "        for index, class_ in enumerate(episodicclasses):\n",
        "            selected = np.random.permutation(numexamples)[:num_shot + num_query]\n",
        "            support[index] = traindataset[class_, selected[:num_shot]]\n",
        "            \n",
        "            # 5 querypoints per classs\n",
        "            query[index] = traindataset[class_, selected[num_shot:]]\n",
        "            \n",
        "        support = np.expand_dims(support, axis=-1)\n",
        "        query = np.expand_dims(query, axis=-1)\n",
        "        labels = np.tile(np.arange(numway)[:, np.newaxis], (1, num_query)).astype(np.uint8)\n",
        "        _, loss_, accuracy_ = sess.run([train, loss, accuracy], feed_dict={support_set: support, query_set: query, y:labels})\n",
        "        \n",
        "        if (episode+1) % 10 == 0:\n",
        "            print('Epoch {} : Episode {} : Loss: {}, Accuracy: {}'.format(epoch+1, episode+1, loss_, accuracy_))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 1 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 1 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 1 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 1 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 1 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 1 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 1 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 1 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 1 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 2 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 2 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 2 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 2 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 2 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 2 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 2 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 2 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 2 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 2 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 3 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 3 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 3 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 3 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 3 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 3 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 3 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 3 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 3 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 3 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 4 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 4 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 4 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 4 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 4 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 4 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 4 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 4 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 4 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 4 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 5 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 5 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 5 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 5 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 5 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 5 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 5 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 5 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 5 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 5 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 6 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 6 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 6 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 6 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 6 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 6 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 6 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 6 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 6 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 6 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 7 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 7 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 7 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 7 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 7 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 7 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 7 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 7 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 7 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 7 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 8 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 8 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 8 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 8 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 8 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 8 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 8 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 8 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 8 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 8 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 9 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 9 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 9 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 9 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 9 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 9 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 9 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 9 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 9 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 9 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 10 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 10 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 10 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 10 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 10 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 10 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 10 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 10 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 10 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 10 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 11 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 11 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 11 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 11 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 11 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 11 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 11 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 11 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 11 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 11 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 12 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 12 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 12 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 12 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 12 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 12 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 12 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 12 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 12 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 12 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 13 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 13 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 13 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 13 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 13 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 13 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 13 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 13 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 13 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 13 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 14 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 14 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 14 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 14 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 14 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 14 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 14 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 14 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 14 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 14 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 15 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 15 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 15 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 15 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 15 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 15 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 15 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 15 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 15 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 15 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 16 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 16 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 16 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 16 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 16 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 16 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 16 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 16 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 16 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 16 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 17 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 17 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 17 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 17 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 17 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 17 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 17 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 17 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 17 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 17 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 18 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 18 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 18 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 18 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 18 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 18 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 18 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 18 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 18 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 18 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 19 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 19 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 19 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 19 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 19 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 19 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 19 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 19 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 19 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 19 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 20 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 20 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 20 : Episode 30 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 20 : Episode 40 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 20 : Episode 50 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 20 : Episode 60 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 20 : Episode 70 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 20 : Episode 80 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 20 : Episode 90 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 20 : Episode 100 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}